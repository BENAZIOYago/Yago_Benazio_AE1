{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "i4dKiZJhIO1_",
        "outputId": "a6a248b7-c648-4ddd-d8a4-76a1e8b3f051"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (NeuralNetwork.py, line 2)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-1059590556>\"\u001b[0;36m, line \u001b[0;32m187\u001b[0;36m, in \u001b[0;35m<cell line: 0>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import NeuralNetwork\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"/content/NeuralNetwork.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    <!-- saved from url=(0113)https://github.com/cmpmech/deep-learning-in-computational-mechanics/blob/main/exercises/chapter3/NeuralNetwork.py -->\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
          ]
        }
      ],
      "source": [
        "### Neural Network Implementation ###\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "\n",
        "class elasticityDataset(Dataset):\n",
        "    def __init__(self, device, numberOfSamples):\n",
        "        self.E = torch.load(\"datasetStrain/E.pt\", map_location=device, weights_only=False)[:numberOfSamples]\n",
        "        self.eps = torch.load(\"datasetStrain/eps.pt\", map_location=device, weights_only=False)[:numberOfSamples]\n",
        "        self.numberOfSamples = len(self.E)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.numberOfSamples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.E[idx], self.eps[idx])\n",
        "\n",
        "\n",
        "class noActivation(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "def makeCNNBlocks(channels, numberOfConvolutionsPerBlock, kernelSize, activation, normalization, skipChannels=None,\n",
        "                  lastActivation=True):\n",
        "    padding = (kernelSize - 1) // 2\n",
        "    if skipChannels == None:\n",
        "        skipChannels = [0 for i in channels]\n",
        "\n",
        "    convolutions = torch.nn.ModuleList()\n",
        "    normalizations = torch.nn.ModuleList()\n",
        "    activations = torch.nn.ModuleList()\n",
        "    for i in range(len(channels) - 1):\n",
        "        for j in range(numberOfConvolutionsPerBlock):\n",
        "            if j == 0:\n",
        "                inChannels = channels[i] + skipChannels[i]\n",
        "            else:\n",
        "                inChannels = channels[i + 1]\n",
        "\n",
        "            convolutions.append(torch.nn.Conv2d(inChannels,\n",
        "                                                channels[i + 1],\n",
        "                                                kernelSize,\n",
        "                                                stride=1,\n",
        "                                                padding=padding))\n",
        "            normalizations.append(normalization(inChannels))\n",
        "\n",
        "            if i == len(channels) - 2 and j == numberOfConvolutionsPerBlock - 1 and lastActivation == False:\n",
        "                activations.append(noActivation())\n",
        "            else:\n",
        "                activations.append(activation())\n",
        "\n",
        "    return convolutions, normalizations, activations\n",
        "\n",
        "\n",
        "class UNet(torch.nn.Module):\n",
        "    def __init__(self, channels, channelsOut, numberOfConvolutionsPerBlock, kernelSize):\n",
        "        super().__init__()\n",
        "        self.kernelSize = kernelSize\n",
        "        self.channels = channels  # channelsIn is defined implicitly\n",
        "        self.channelsOut = channelsOut\n",
        "        self.numberOfConvolutionsPerBlock = numberOfConvolutionsPerBlock\n",
        "\n",
        "        self.numberOfBottleNeckLayers = self.numberOfConvolutionsPerBlock\n",
        "        self.channelsDown = copy.deepcopy(self.channels)\n",
        "        self.channelsUp = copy.deepcopy(self.channels)[::-1]\n",
        "        self.channelsUp[-1] = self.channelsOut\n",
        "\n",
        "        self.activation = lambda: torch.nn.LeakyReLU(inplace=True)\n",
        "        self.normalization = lambda s: torch.nn.BatchNorm2d(s)\n",
        "\n",
        "        # downsampling\n",
        "        self.convolutionsDown, self.normalizationsDown, self.activationsDown = makeCNNBlocks(self.channelsDown,\n",
        "                                                                                             self.numberOfConvolutionsPerBlock,\n",
        "                                                                                             self.kernelSize,\n",
        "                                                                                             self.activation,\n",
        "                                                                                             self.normalization)\n",
        "        self.downsample = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # bottleneck\n",
        "        self.convolutionsBottleneck, self.normalizationsBottleneck, self.activationsBottleneck = makeCNNBlocks(\n",
        "            [self.channelsDown[-1], self.channelsUp[0]],\n",
        "            self.numberOfBottleNeckLayers,\n",
        "            self.kernelSize,\n",
        "            self.activation,\n",
        "            self.normalization)\n",
        "\n",
        "        # upsampling\n",
        "        skipChannels = self.channelsDown[1:][::-1]\n",
        "        skipChannels[-1] += 1\n",
        "        self.convolutionsUp, self.normalizationsUp, self.activationsUp = makeCNNBlocks(self.channelsUp,\n",
        "                                                                                       self.numberOfConvolutionsPerBlock,\n",
        "                                                                                       self.kernelSize,\n",
        "                                                                                       self.activation,\n",
        "                                                                                       self.normalization,\n",
        "                                                                                       skipChannels=skipChannels,\n",
        "                                                                                       lastActivation=False)\n",
        "        self.upsample = torch.nn.Upsample(scale_factor=2, mode='bilinear')  # could be changed to 'nearest'\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x\n",
        "        x_ = []  # skip connections\n",
        "\n",
        "        for i in range(len(self.channelsDown) - 1):\n",
        "            for j in range(self.numberOfConvolutionsPerBlock):\n",
        "                index = i * self.numberOfConvolutionsPerBlock + j\n",
        "                x = self.activationsDown[index](self.convolutionsDown[index](self.normalizationsDown[index](x)))\n",
        "            x_.append(x)\n",
        "            x = self.downsample(x)\n",
        "\n",
        "        for j in range(self.numberOfBottleNeckLayers):\n",
        "            index = j\n",
        "            x = self.activationsBottleneck[index](\n",
        "                self.convolutionsBottleneck[index](self.normalizationsBottleneck[index](x)))\n",
        "\n",
        "        for i in range(len(self.channelsUp) - 1):\n",
        "            x = torch.cat((self.upsample(x), x_[-(i + 1)]), 1)  # concatenate for skip connections\n",
        "            if i == len(self.channelsUp) - 2:\n",
        "                x = torch.cat((x, x0), 1)\n",
        "            for j in range(self.numberOfConvolutionsPerBlock):\n",
        "                index = i * self.numberOfConvolutionsPerBlock + j\n",
        "                x = self.activationsUp[index](self.convolutionsUp[index](self.normalizationsUp[index](x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedforwardCNN(torch.nn.Module):\n",
        "    def __init__(self, channels, channelsOut, kernelSize):\n",
        "        super().__init__()\n",
        "        self.kernelSize = kernelSize\n",
        "        self.channels = channels + [channelsOut]  # channelsIn is defined implicitly\n",
        "        #        self.channelsOut = channelsOut\n",
        "\n",
        "        self.activation = lambda: torch.nn.LeakyReLU(inplace=True)\n",
        "        self.normalization = lambda s: torch.nn.BatchNorm2d(s)\n",
        "\n",
        "        self.convolutions, self.normalizations, self.activations = makeCNNBlocks(self.channels, 1, kernelSize,\n",
        "                                                                                 self.activation, self.normalization,\n",
        "                                                                                 lastActivation=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.channels) - 1):\n",
        "            x = self.activations[i](self.convolutions[i](self.normalizations[i](x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNetWithSubsequentFeedforwardCNN(torch.nn.Module):\n",
        "    def __init__(self, channelsUNet, numberOfConvolutionsPerBlockUNet, channelsFeedforwardCNN, channelsOut, kernelSize):\n",
        "        super().__init__()\n",
        "        self.uNet = UNet(channelsUNet, channelsFeedforwardCNN[0], numberOfConvolutionsPerBlockUNet, kernelSize)\n",
        "        self.feedforwardCNN = FeedforwardCNN(channelsFeedforwardCNN, channelsOut, kernelSize)\n",
        "\n",
        "        self.activation = torch.nn.LeakyReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.uNet(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.feedforwardCNN(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def initWeights(m):\n",
        "    \"\"\"Initialize weights of neural network with xavier initialization.\"\"\"\n",
        "    if type(m) == torch.nn.Linear or type(m) == torch.nn.Conv2d or type(m) == torch.nn.Conv3d:\n",
        "        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "\n",
        "def costFunction(prediction, label):\n",
        "    return torch.mean((prediction - label) ** 2)\n",
        "\n",
        "### Fim ###\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary           # torchinfo replaces torchsummary, uses same syntax\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import NeuralNetwork\n",
        "\n",
        "#CNN for strain distribution defined in NeuralNetwork.py\n",
        "import datetime\n",
        "import copy\n",
        "import torchvision\n",
        "\n",
        "#User settings\n",
        "#tensorBoard options\n",
        "\n",
        "\n",
        "writeGraph = False\n",
        "writeHistogram = False\n",
        "writeLearningHistory = False\n",
        "writePredictions = False\n",
        "\n",
        "#driver setup\n",
        "\n",
        "\n",
        "seed = 2\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "writer = SummaryWriter(log_dir=\"./logs/fit\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "#neural network architecture selection\n",
        "\n",
        "\n",
        "selectNN = 2  # 0 for UNet, 1 for sequential CNN, 2 for UNet with subsequent feedforward CNN\n",
        "\n",
        "#model parameters\n",
        "\n",
        "\n",
        "kernelSize = 3\n",
        "\n",
        "if selectNN == 0:\n",
        "    channels = [1, 32, 64]\n",
        "    channelsOut = 3\n",
        "    numberOfConvolutionsPerBlock = 1\n",
        "    model = NeuralNetwork.UNet(channels, channelsOut, numberOfConvolutionsPerBlock, kernelSize)\n",
        "elif selectNN == 1:\n",
        "    channels = [1, 32, 64, 32]\n",
        "    channelsOut = 3\n",
        "    model = NeuralNetwork.FeedforwardCNN(channels, channelsOut, kernelSize)\n",
        "elif selectNN == 2:\n",
        "    channelsUNet = [1, 32, 64]\n",
        "    numberOfConvolutionsPerBlockUNet = 1\n",
        "    channelsFeedforwardCNN = [64, 32, 16]\n",
        "    channelsOut = 3\n",
        "    model = NeuralNetwork.UNetWithSubsequentFeedforwardCNN(channelsUNet, numberOfConvolutionsPerBlockUNet,\n",
        "                                                           channelsFeedforwardCNN, channelsOut, kernelSize)\n",
        "model.to(device)\n",
        "summary(model, (1, 1, 32, 32))\n",
        "if writeGraph == True:\n",
        "    writer.add_graph(model, torch.randn((1, 1, 32, 32), device=device))\n",
        "\n",
        "#hyperparameters\n",
        "\n",
        "\n",
        "batchSize = 128\n",
        "alpha = -0.2\n",
        "beta = 0.2\n",
        "weightDecay = 0\n",
        "lr = 2e-3\n",
        "epochs = 1000\n",
        "earlyStopping = True\n",
        "\n",
        "#Pre-processing\n",
        "#prepare dataset\n",
        "\n",
        "\n",
        "numberOfTrainingSamples = 1\n",
        "numberOfSamples = numberOfTrainingSamples + 32\n",
        "dataset = NeuralNetwork.elasticityDataset(device, numberOfSamples)\n",
        "# normalization\n",
        "dataset.E = (dataset.E - np.mean([3000, 85000])) / np.std([3000, 85000])\n",
        "datasetTraining, datasetValidation = torch.utils.data.random_split(dataset, [numberOfTrainingSamples,\n",
        "                                                                             len(dataset) - numberOfTrainingSamples],\n",
        "                                                                   generator=torch.Generator().manual_seed(2))\n",
        "\n",
        "dataloaderTraining = DataLoader(datasetTraining, batch_size=batchSize)\n",
        "dataloaderValidation = DataLoader(datasetValidation, batch_size=len(dataset))\n",
        "\n",
        "if writePredictions == True:\n",
        "    sample = next(iter(dataloaderValidation))\n",
        "    validationLabelImages = torchvision.utils.make_grid(sample[1][:16], normalize=True, value_range=(-1, 1))\n",
        "    for i in range(3):\n",
        "        writer.add_image(f'validation label {i + 1}', validationLabelImages[i], dataformats='HW')\n",
        "\n",
        "\n",
        "#Training\n",
        "#optimizer and scheduler instantiation\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
        "lr_lambda = lambda epoch: (beta * epoch + 1) ** alpha\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "#history variables\n",
        "\n",
        "\n",
        "trainingCostHistory = np.zeros(epochs)\n",
        "validationCostHistory = np.zeros(epochs)\n",
        "\n",
        "#training loop\n",
        "\n",
        "\n",
        "start = time.perf_counter()\n",
        "start0 = start\n",
        "bestCost = 1e10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    if writeHistogram == True:\n",
        "        for name, param in model.named_parameters():\n",
        "            writer.add_histogram(name, param, epoch)\n",
        "\n",
        "    model.train()\n",
        "    for batch, sample in enumerate(dataloaderTraining):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        prediction = model(sample[0])\n",
        "        cost = NeuralNetwork.costFunction(prediction, sample[1])\n",
        "\n",
        "        trainingCostHistory[epoch] += cost.detach() * len(sample[1])\n",
        "\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    trainingCostHistory[epoch] /= numberOfTrainingSamples\n",
        "    scheduler.step()\n",
        "    if writeHistogram == True:\n",
        "        for name, param in model.named_parameters():\n",
        "            writer.add_histogram(f'{name}.grad', param.grad, epoch)\n",
        "\n",
        "    model.eval()\n",
        "    sample = next(iter(dataloaderValidation))\n",
        "    with torch.no_grad():\n",
        "        prediction = model(sample[0])\n",
        "        cost = NeuralNetwork.costFunction(prediction, sample[1])\n",
        "\n",
        "        validationCostHistory[epoch] = cost\n",
        "        if validationCostHistory[epoch] < bestCost:\n",
        "            modelParametersBest = copy.deepcopy(model.state_dict())\n",
        "            bestCost = validationCostHistory[epoch]\n",
        "\n",
        "        if writePredictions == True:\n",
        "            validationPredictionImages = torchvision.utils.make_grid(prediction[:16], normalize=True,\n",
        "                                                                     value_range=(-1, 1))\n",
        "            for i in range(3):\n",
        "                writer.add_image(f'validation prediction {i + 1}', validationPredictionImages[i], epoch,\n",
        "                                 dataformats='HW')\n",
        "\n",
        "    elapsedTime = time.perf_counter() - start\n",
        "    if epoch % 10 == 0:\n",
        "        string = \"Epoch: {}/{}\\t\\tTraining cost: {:.2e}\\t\\tValidation cost: {:.2e}\\nElapsed time for last epoch: {:.2f} s\"\n",
        "        print(string.format(epoch + 1, epochs, trainingCostHistory[epoch], validationCostHistory[epoch], elapsedTime))\n",
        "    start = time.perf_counter()\n",
        "\n",
        "    if writeLearningHistory == True:\n",
        "        writer.add_scalar('training_loss', trainingCostHistory[epoch], epoch)\n",
        "        writer.add_scalar('validation_loss', validationCostHistory[epoch], epoch)\n",
        "\n",
        "if earlyStopping == True:\n",
        "    model.load_state_dict(modelParametersBest)\n",
        "print(\"Total elapsed time during training: {:.2f} s\".format(time.perf_counter() - start0))\n",
        "writer.close()"
      ]
    }
  ]
}